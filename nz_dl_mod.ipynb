{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nz_dl_mod.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNz62Oy3EFd/Xi62/2w+i+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/widnu/Predictive_Policing_DL/blob/master/nz_dl_mod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74owbDLPHimY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66ba8af7-8948-4982-8107-f12302031274"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DLWazIzycWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv('gdrive/My Drive/Colab Notebooks/dataset/nz_crime_dataset.csv', encoding='utf-8-sig')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTYiXGH0Txt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "outputId": "7a29cbc1-cdf0-4b61-fee9-05a47771b23a"
      },
      "source": [
        "df.info()\n",
        "df.describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1167347 entries, 0 to 1167346\n",
            "Data columns (total 19 columns):\n",
            " #   Column                     Non-Null Count    Dtype  \n",
            "---  ------                     --------------    -----  \n",
            " 0   DATE_NO_TIME               1167347 non-null  object \n",
            " 1   DATE_TIME                  1167347 non-null  object \n",
            " 2   DAY_AREA_CRIME_COUNT_ROLL  1167347 non-null  float64\n",
            " 3   DAY_AREA_CRIME_COUNT       1167347 non-null  int64  \n",
            " 4   MONTH_AREA_CRIME_COUNT     1167347 non-null  int64  \n",
            " 5   YEAR_AREA_CRIME_COUNT      1167347 non-null  int64  \n",
            " 6   TIME_SINCE_LAST_CRIME      1167347 non-null  float64\n",
            " 7   MONTH                      1167347 non-null  int64  \n",
            " 8   QUARTER                    1167347 non-null  float64\n",
            " 9   DAY_OF_WEEK                1167347 non-null  object \n",
            " 10  DAY                        1167347 non-null  float64\n",
            " 11  HOUR                       1167347 non-null  float64\n",
            " 12  HOUR_PARTITION             1167347 non-null  float64\n",
            " 13  MESHBLOCK                  1167347 non-null  int64  \n",
            " 14  AREA_0                     1167347 non-null  object \n",
            " 15  AREA_1                     1167347 non-null  object \n",
            " 16  WEAPON_TYPE                1167347 non-null  int64  \n",
            " 17  CRIME_TYPE                 1167347 non-null  object \n",
            " 18  RISK                       1167347 non-null  float64\n",
            "dtypes: float64(7), int64(6), object(6)\n",
            "memory usage: 169.2+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DAY_AREA_CRIME_COUNT_ROLL</th>\n",
              "      <th>DAY_AREA_CRIME_COUNT</th>\n",
              "      <th>MONTH_AREA_CRIME_COUNT</th>\n",
              "      <th>YEAR_AREA_CRIME_COUNT</th>\n",
              "      <th>TIME_SINCE_LAST_CRIME</th>\n",
              "      <th>MONTH</th>\n",
              "      <th>QUARTER</th>\n",
              "      <th>DAY</th>\n",
              "      <th>HOUR</th>\n",
              "      <th>HOUR_PARTITION</th>\n",
              "      <th>MESHBLOCK</th>\n",
              "      <th>WEAPON_TYPE</th>\n",
              "      <th>RISK</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.740339e+04</td>\n",
              "      <td>4.102593e+03</td>\n",
              "      <td>3.252062e+01</td>\n",
              "      <td>3.397887e+02</td>\n",
              "      <td>1.281016e-01</td>\n",
              "      <td>6.536036e+00</td>\n",
              "      <td>2.512512e+00</td>\n",
              "      <td>4.694526e+00</td>\n",
              "      <td>1.310084e+01</td>\n",
              "      <td>1.205440e+01</td>\n",
              "      <td>1.302863e+06</td>\n",
              "      <td>9.731468e-03</td>\n",
              "      <td>1.004760e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.485673e+03</td>\n",
              "      <td>2.827224e+03</td>\n",
              "      <td>3.752400e+01</td>\n",
              "      <td>4.161401e+02</td>\n",
              "      <td>7.455829e+00</td>\n",
              "      <td>3.552330e+00</td>\n",
              "      <td>1.149184e+00</td>\n",
              "      <td>1.891168e+00</td>\n",
              "      <td>4.183061e+00</td>\n",
              "      <td>4.167649e+00</td>\n",
              "      <td>8.577749e+05</td>\n",
              "      <td>1.042445e-01</td>\n",
              "      <td>7.758172e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.418400e+04</td>\n",
              "      <td>1.179000e+03</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-1.461000e+03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.655300e+04</td>\n",
              "      <td>1.628000e+03</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>9.300000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>1.262178e+01</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>6.075000e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.718400e+04</td>\n",
              "      <td>1.971000e+03</td>\n",
              "      <td>1.900000e+01</td>\n",
              "      <td>1.960000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>6.000000e+00</td>\n",
              "      <td>1.317855e+01</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>1.167000e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.789000e+04</td>\n",
              "      <td>7.184000e+03</td>\n",
              "      <td>3.900000e+01</td>\n",
              "      <td>3.970000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>6.000000e+00</td>\n",
              "      <td>1.400000e+01</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>2.029200e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.263600e+04</td>\n",
              "      <td>8.902000e+03</td>\n",
              "      <td>2.760000e+02</td>\n",
              "      <td>2.294000e+03</td>\n",
              "      <td>5.770000e+02</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>2.300000e+01</td>\n",
              "      <td>2.100000e+01</td>\n",
              "      <td>3.210003e+06</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>2.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       DAY_AREA_CRIME_COUNT_ROLL  ...          RISK\n",
              "count               1.167347e+06  ...  1.167347e+06\n",
              "mean                1.740339e+04  ...  1.004760e+00\n",
              "std                 1.485673e+03  ...  7.758172e-01\n",
              "min                 1.418400e+04  ...  0.000000e+00\n",
              "25%                 1.655300e+04  ...  0.000000e+00\n",
              "50%                 1.718400e+04  ...  1.000000e+00\n",
              "75%                 1.789000e+04  ...  2.000000e+00\n",
              "max                 2.263600e+04  ...  2.000000e+00\n",
              "\n",
              "[8 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FVtG3Ia1e09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "e828e2d4-571d-433f-b61f-e5ab91ec61f4"
      },
      "source": [
        "#######################################################\n",
        "# Count total NaN at each column in DataFrame\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Count all NaN in a DataFrame (both columns & Rows)\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "# erase every row (axis=0) that has \"any\" Null value in it.\n",
        "df = df.dropna(how='any',axis=0) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATE_NO_TIME                 0\n",
            "DATE_TIME                    0\n",
            "DAY_AREA_CRIME_COUNT_ROLL    0\n",
            "DAY_AREA_CRIME_COUNT         0\n",
            "MONTH_AREA_CRIME_COUNT       0\n",
            "YEAR_AREA_CRIME_COUNT        0\n",
            "TIME_SINCE_LAST_CRIME        0\n",
            "MONTH                        0\n",
            "QUARTER                      0\n",
            "DAY_OF_WEEK                  0\n",
            "DAY                          0\n",
            "HOUR                         0\n",
            "HOUR_PARTITION               0\n",
            "MESHBLOCK                    0\n",
            "AREA_0                       0\n",
            "AREA_1                       0\n",
            "WEAPON_TYPE                  0\n",
            "CRIME_TYPE                   0\n",
            "RISK                         0\n",
            "dtype: int64\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAzMTKyP1kOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################\n",
        "feature_var = ['DAY_AREA_CRIME_COUNT_ROLL', 'MONTH', 'DAY', 'QUARTER', 'HOUR_PARTITION', 'AREA_0', 'AREA_1', 'WEAPON_TYPE', 'CRIME_TYPE']\n",
        "response_var = 'RISK'\n",
        "#######################################################\n",
        "X = df[feature_var]\n",
        "y = df.pop(response_var)\n",
        "# y = df[response_var]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWbzwzPw1sYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "2c9611fe-f901-468a-997b-0328ef2efa19"
      },
      "source": [
        "########################################\n",
        "# Encode categorical variables\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oe = OrdinalEncoder()\n",
        "oe.fit(X)\n",
        "X = oe.transform(X)\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA(n_components=5)\n",
        "# X = pca.fit_transform(X)\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc = StandardScaler()\n",
        "# X = sc.fit_transform(X)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "y = le.transform(y)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# from keras.utils import to_categorical\n",
        "# y = to_categorical(y)\n",
        "\n",
        "#######################################################\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "\n",
        "#######################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 5)\n",
        "\n",
        "# Reserve 10,000 samples for validation\n",
        "X_val = X_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "X_train = X_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "#######################################################\n",
        "#df_X = pd.DataFrame(data=X)\n",
        "#df_y = pd.DataFrame(data=y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1: 464704, 2: 354100, 0: 348543})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Counter({0: 464704, 1: 464704, 2: 464704})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNmYLYTX1yr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "#dataset = tf.data.Dataset.from_tensor_slices((df_X.values, df_y.values))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6GTytTP2Va0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_dataset = dataset.shuffle(len(df_X)).batch(1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMcBPgWI2OQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_compiled_model():\n",
        "    model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6P7I7Nx2mcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "975818ef-d934-4431-aa86-9c7f124b5306"
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=256,\n",
        "    validation_data=(X_val, y_val),\n",
        ")\n",
        "\n",
        "history.history\n",
        "model.summary()\n",
        "\n",
        "# https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/\n",
        "\n",
        "##################################################\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(X_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "# Generate predictions (probabilities -- the output of the last layer)\n",
        "# on new data using `predict`\n",
        "print(\"Generate predictions for 3 samples\")\n",
        "predictions = model.predict(X_test[:3])\n",
        "print(\"predictions shape:\", predictions.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.9698 - accuracy: 0.5128 - val_loss: 0.9043 - val_accuracy: 0.5635\n",
            "Epoch 2/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.8696 - accuracy: 0.5802 - val_loss: 0.8509 - val_accuracy: 0.5926\n",
            "Epoch 3/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.8268 - accuracy: 0.6058 - val_loss: 0.8086 - val_accuracy: 0.6153\n",
            "Epoch 4/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7997 - accuracy: 0.6220 - val_loss: 0.7939 - val_accuracy: 0.6213\n",
            "Epoch 5/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7794 - accuracy: 0.6340 - val_loss: 0.7802 - val_accuracy: 0.6341\n",
            "Epoch 6/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7653 - accuracy: 0.6413 - val_loss: 0.7747 - val_accuracy: 0.6346\n",
            "Epoch 7/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.7539 - accuracy: 0.6475 - val_loss: 0.7758 - val_accuracy: 0.6351\n",
            "Epoch 8/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7459 - accuracy: 0.6518 - val_loss: 0.7427 - val_accuracy: 0.6538\n",
            "Epoch 9/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7398 - accuracy: 0.6557 - val_loss: 0.7432 - val_accuracy: 0.6469\n",
            "Epoch 10/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.7334 - accuracy: 0.6595 - val_loss: 0.7309 - val_accuracy: 0.6641\n",
            "Epoch 11/256\n",
            "15092/15092 [==============================] - 44s 3ms/step - loss: 0.7286 - accuracy: 0.6624 - val_loss: 0.7241 - val_accuracy: 0.6603\n",
            "Epoch 12/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7238 - accuracy: 0.6654 - val_loss: 0.7297 - val_accuracy: 0.6610\n",
            "Epoch 13/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7184 - accuracy: 0.6682 - val_loss: 0.7139 - val_accuracy: 0.6645\n",
            "Epoch 14/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.7128 - accuracy: 0.6715 - val_loss: 0.7362 - val_accuracy: 0.6633\n",
            "Epoch 15/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.7090 - accuracy: 0.6739 - val_loss: 0.7026 - val_accuracy: 0.6722\n",
            "Epoch 16/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.7035 - accuracy: 0.6775 - val_loss: 0.6996 - val_accuracy: 0.6771\n",
            "Epoch 17/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6993 - accuracy: 0.6804 - val_loss: 0.7228 - val_accuracy: 0.6687\n",
            "Epoch 18/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6951 - accuracy: 0.6818 - val_loss: 0.7068 - val_accuracy: 0.6753\n",
            "Epoch 19/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6926 - accuracy: 0.6835 - val_loss: 0.6861 - val_accuracy: 0.6856\n",
            "Epoch 20/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6889 - accuracy: 0.6854 - val_loss: 0.6873 - val_accuracy: 0.6830\n",
            "Epoch 21/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6866 - accuracy: 0.6866 - val_loss: 0.7038 - val_accuracy: 0.6790\n",
            "Epoch 22/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6843 - accuracy: 0.6877 - val_loss: 0.7048 - val_accuracy: 0.6747\n",
            "Epoch 23/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6801 - accuracy: 0.6902 - val_loss: 0.6975 - val_accuracy: 0.6826\n",
            "Epoch 24/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6778 - accuracy: 0.6909 - val_loss: 0.6754 - val_accuracy: 0.6905\n",
            "Epoch 25/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6771 - accuracy: 0.6916 - val_loss: 0.6845 - val_accuracy: 0.6797\n",
            "Epoch 26/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6741 - accuracy: 0.6934 - val_loss: 0.6824 - val_accuracy: 0.6847\n",
            "Epoch 27/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6720 - accuracy: 0.6941 - val_loss: 0.6801 - val_accuracy: 0.6904\n",
            "Epoch 28/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6699 - accuracy: 0.6951 - val_loss: 0.6892 - val_accuracy: 0.6769\n",
            "Epoch 29/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6669 - accuracy: 0.6968 - val_loss: 0.6605 - val_accuracy: 0.6940\n",
            "Epoch 30/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6651 - accuracy: 0.6976 - val_loss: 0.6582 - val_accuracy: 0.6978\n",
            "Epoch 31/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6652 - accuracy: 0.6976 - val_loss: 0.6807 - val_accuracy: 0.6853\n",
            "Epoch 32/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6631 - accuracy: 0.6989 - val_loss: 0.7048 - val_accuracy: 0.6737\n",
            "Epoch 33/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6606 - accuracy: 0.7002 - val_loss: 0.6530 - val_accuracy: 0.6990\n",
            "Epoch 34/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6584 - accuracy: 0.7007 - val_loss: 0.6670 - val_accuracy: 0.6957\n",
            "Epoch 35/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6573 - accuracy: 0.7015 - val_loss: 0.6648 - val_accuracy: 0.6999\n",
            "Epoch 36/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6562 - accuracy: 0.7020 - val_loss: 0.6528 - val_accuracy: 0.7008\n",
            "Epoch 37/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6544 - accuracy: 0.7028 - val_loss: 0.6567 - val_accuracy: 0.6987\n",
            "Epoch 38/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6534 - accuracy: 0.7036 - val_loss: 0.6771 - val_accuracy: 0.6877\n",
            "Epoch 39/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6530 - accuracy: 0.7034 - val_loss: 0.6531 - val_accuracy: 0.7044\n",
            "Epoch 40/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6504 - accuracy: 0.7050 - val_loss: 0.6608 - val_accuracy: 0.7032\n",
            "Epoch 41/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6495 - accuracy: 0.7055 - val_loss: 0.6552 - val_accuracy: 0.6991\n",
            "Epoch 42/256\n",
            "15092/15092 [==============================] - 44s 3ms/step - loss: 0.6480 - accuracy: 0.7062 - val_loss: 0.6671 - val_accuracy: 0.6983\n",
            "Epoch 43/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6470 - accuracy: 0.7066 - val_loss: 0.6745 - val_accuracy: 0.6890\n",
            "Epoch 44/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6463 - accuracy: 0.7066 - val_loss: 0.6799 - val_accuracy: 0.6796\n",
            "Epoch 45/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6453 - accuracy: 0.7074 - val_loss: 0.6518 - val_accuracy: 0.7011\n",
            "Epoch 46/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6434 - accuracy: 0.7083 - val_loss: 0.6424 - val_accuracy: 0.7054\n",
            "Epoch 47/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6429 - accuracy: 0.7085 - val_loss: 0.6426 - val_accuracy: 0.7095\n",
            "Epoch 48/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6422 - accuracy: 0.7092 - val_loss: 0.6472 - val_accuracy: 0.7032\n",
            "Epoch 49/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6414 - accuracy: 0.7096 - val_loss: 0.6412 - val_accuracy: 0.7092\n",
            "Epoch 50/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6404 - accuracy: 0.7100 - val_loss: 0.6294 - val_accuracy: 0.7134\n",
            "Epoch 51/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6396 - accuracy: 0.7106 - val_loss: 0.6399 - val_accuracy: 0.7090\n",
            "Epoch 52/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6385 - accuracy: 0.7108 - val_loss: 0.6562 - val_accuracy: 0.7050\n",
            "Epoch 53/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6382 - accuracy: 0.7112 - val_loss: 0.6451 - val_accuracy: 0.7073\n",
            "Epoch 54/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6371 - accuracy: 0.7118 - val_loss: 0.6469 - val_accuracy: 0.7033\n",
            "Epoch 55/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6366 - accuracy: 0.7119 - val_loss: 0.6394 - val_accuracy: 0.7085\n",
            "Epoch 56/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6355 - accuracy: 0.7123 - val_loss: 0.6428 - val_accuracy: 0.7065\n",
            "Epoch 57/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6351 - accuracy: 0.7129 - val_loss: 0.6412 - val_accuracy: 0.7063\n",
            "Epoch 58/256\n",
            "15092/15092 [==============================] - 46s 3ms/step - loss: 0.6340 - accuracy: 0.7130 - val_loss: 0.6324 - val_accuracy: 0.7096\n",
            "Epoch 59/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6333 - accuracy: 0.7138 - val_loss: 0.6432 - val_accuracy: 0.7040\n",
            "Epoch 60/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6317 - accuracy: 0.7142 - val_loss: 0.6355 - val_accuracy: 0.7107\n",
            "Epoch 61/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6322 - accuracy: 0.7143 - val_loss: 0.6251 - val_accuracy: 0.7211\n",
            "Epoch 62/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6309 - accuracy: 0.7148 - val_loss: 0.6587 - val_accuracy: 0.6994\n",
            "Epoch 63/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6308 - accuracy: 0.7150 - val_loss: 0.6280 - val_accuracy: 0.7130\n",
            "Epoch 64/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6292 - accuracy: 0.7156 - val_loss: 0.6286 - val_accuracy: 0.7103\n",
            "Epoch 65/256\n",
            "15092/15092 [==============================] - 47s 3ms/step - loss: 0.6291 - accuracy: 0.7153 - val_loss: 0.6267 - val_accuracy: 0.7147\n",
            "Epoch 66/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6293 - accuracy: 0.7155 - val_loss: 0.6280 - val_accuracy: 0.7127\n",
            "Epoch 67/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6285 - accuracy: 0.7161 - val_loss: 0.6247 - val_accuracy: 0.7156\n",
            "Epoch 68/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6275 - accuracy: 0.7166 - val_loss: 0.6486 - val_accuracy: 0.7044\n",
            "Epoch 69/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6264 - accuracy: 0.7171 - val_loss: 0.6334 - val_accuracy: 0.7146\n",
            "Epoch 70/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6261 - accuracy: 0.7176 - val_loss: 0.6387 - val_accuracy: 0.7096\n",
            "Epoch 71/256\n",
            "15092/15092 [==============================] - 44s 3ms/step - loss: 0.6255 - accuracy: 0.7175 - val_loss: 0.6521 - val_accuracy: 0.7055\n",
            "Epoch 72/256\n",
            "15092/15092 [==============================] - 48s 3ms/step - loss: 0.6248 - accuracy: 0.7180 - val_loss: 0.6305 - val_accuracy: 0.7125\n",
            "Epoch 73/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6248 - accuracy: 0.7179 - val_loss: 0.6287 - val_accuracy: 0.7167\n",
            "Epoch 74/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6239 - accuracy: 0.7180 - val_loss: 0.6809 - val_accuracy: 0.6909\n",
            "Epoch 75/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6229 - accuracy: 0.7188 - val_loss: 0.6327 - val_accuracy: 0.7089\n",
            "Epoch 76/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6229 - accuracy: 0.7185 - val_loss: 0.6201 - val_accuracy: 0.7174\n",
            "Epoch 77/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6221 - accuracy: 0.7191 - val_loss: 0.6288 - val_accuracy: 0.7168\n",
            "Epoch 78/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6217 - accuracy: 0.7197 - val_loss: 0.6289 - val_accuracy: 0.7182\n",
            "Epoch 79/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6210 - accuracy: 0.7195 - val_loss: 0.6492 - val_accuracy: 0.7064\n",
            "Epoch 80/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6216 - accuracy: 0.7195 - val_loss: 0.6310 - val_accuracy: 0.7109\n",
            "Epoch 81/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6209 - accuracy: 0.7193 - val_loss: 0.6290 - val_accuracy: 0.7131\n",
            "Epoch 82/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6200 - accuracy: 0.7203 - val_loss: 0.6201 - val_accuracy: 0.7191\n",
            "Epoch 83/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6191 - accuracy: 0.7202 - val_loss: 0.6448 - val_accuracy: 0.7093\n",
            "Epoch 84/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6183 - accuracy: 0.7209 - val_loss: 0.6226 - val_accuracy: 0.7204\n",
            "Epoch 85/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6202 - accuracy: 0.7206 - val_loss: 0.6233 - val_accuracy: 0.7202\n",
            "Epoch 86/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6181 - accuracy: 0.7215 - val_loss: 0.6191 - val_accuracy: 0.7141\n",
            "Epoch 87/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6181 - accuracy: 0.7211 - val_loss: 0.6465 - val_accuracy: 0.7041\n",
            "Epoch 88/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6169 - accuracy: 0.7218 - val_loss: 0.6124 - val_accuracy: 0.7221\n",
            "Epoch 89/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6162 - accuracy: 0.7217 - val_loss: 0.6218 - val_accuracy: 0.7227\n",
            "Epoch 90/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6168 - accuracy: 0.7219 - val_loss: 0.6234 - val_accuracy: 0.7157\n",
            "Epoch 91/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6169 - accuracy: 0.7218 - val_loss: 0.6135 - val_accuracy: 0.7192\n",
            "Epoch 92/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6164 - accuracy: 0.7218 - val_loss: 0.6320 - val_accuracy: 0.7157\n",
            "Epoch 93/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6155 - accuracy: 0.7225 - val_loss: 0.6635 - val_accuracy: 0.7024\n",
            "Epoch 94/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6147 - accuracy: 0.7230 - val_loss: 0.6303 - val_accuracy: 0.7100\n",
            "Epoch 95/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6153 - accuracy: 0.7224 - val_loss: 0.6256 - val_accuracy: 0.7171\n",
            "Epoch 96/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6143 - accuracy: 0.7230 - val_loss: 0.6629 - val_accuracy: 0.7056\n",
            "Epoch 97/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6137 - accuracy: 0.7231 - val_loss: 0.6212 - val_accuracy: 0.7168\n",
            "Epoch 98/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6129 - accuracy: 0.7234 - val_loss: 0.6295 - val_accuracy: 0.7103\n",
            "Epoch 99/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6139 - accuracy: 0.7233 - val_loss: 0.6324 - val_accuracy: 0.7067\n",
            "Epoch 100/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6131 - accuracy: 0.7237 - val_loss: 0.6130 - val_accuracy: 0.7216\n",
            "Epoch 101/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6122 - accuracy: 0.7239 - val_loss: 0.6158 - val_accuracy: 0.7185\n",
            "Epoch 102/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.6114 - accuracy: 0.7243 - val_loss: 0.6326 - val_accuracy: 0.7142\n",
            "Epoch 103/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6131 - accuracy: 0.7237 - val_loss: 0.6274 - val_accuracy: 0.7195\n",
            "Epoch 104/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6117 - accuracy: 0.7241 - val_loss: 0.6289 - val_accuracy: 0.7149\n",
            "Epoch 105/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6107 - accuracy: 0.7247 - val_loss: 0.6146 - val_accuracy: 0.7223\n",
            "Epoch 106/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6108 - accuracy: 0.7242 - val_loss: 0.6508 - val_accuracy: 0.7056\n",
            "Epoch 107/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6105 - accuracy: 0.7245 - val_loss: 0.6169 - val_accuracy: 0.7216\n",
            "Epoch 108/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6110 - accuracy: 0.7244 - val_loss: 0.6158 - val_accuracy: 0.7239\n",
            "Epoch 109/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6103 - accuracy: 0.7246 - val_loss: 0.6087 - val_accuracy: 0.7251\n",
            "Epoch 110/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6095 - accuracy: 0.7247 - val_loss: 0.6388 - val_accuracy: 0.7113\n",
            "Epoch 111/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6089 - accuracy: 0.7255 - val_loss: 0.6209 - val_accuracy: 0.7214\n",
            "Epoch 112/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6087 - accuracy: 0.7253 - val_loss: 0.6098 - val_accuracy: 0.7222\n",
            "Epoch 113/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6086 - accuracy: 0.7252 - val_loss: 0.6096 - val_accuracy: 0.7230\n",
            "Epoch 114/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6091 - accuracy: 0.7251 - val_loss: 0.6313 - val_accuracy: 0.7124\n",
            "Epoch 115/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6077 - accuracy: 0.7261 - val_loss: 0.6199 - val_accuracy: 0.7162\n",
            "Epoch 116/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6079 - accuracy: 0.7258 - val_loss: 0.6298 - val_accuracy: 0.7131\n",
            "Epoch 117/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6072 - accuracy: 0.7263 - val_loss: 0.6147 - val_accuracy: 0.7223\n",
            "Epoch 118/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6083 - accuracy: 0.7261 - val_loss: 0.6220 - val_accuracy: 0.7138\n",
            "Epoch 119/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6059 - accuracy: 0.7267 - val_loss: 0.6117 - val_accuracy: 0.7193\n",
            "Epoch 120/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6070 - accuracy: 0.7264 - val_loss: 0.6252 - val_accuracy: 0.7128\n",
            "Epoch 121/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6067 - accuracy: 0.7266 - val_loss: 0.6159 - val_accuracy: 0.7222\n",
            "Epoch 122/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6061 - accuracy: 0.7267 - val_loss: 0.6149 - val_accuracy: 0.7230\n",
            "Epoch 123/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6052 - accuracy: 0.7270 - val_loss: 0.6087 - val_accuracy: 0.7251\n",
            "Epoch 124/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6062 - accuracy: 0.7267 - val_loss: 0.6045 - val_accuracy: 0.7270\n",
            "Epoch 125/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6048 - accuracy: 0.7275 - val_loss: 0.6108 - val_accuracy: 0.7265\n",
            "Epoch 126/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6050 - accuracy: 0.7267 - val_loss: 0.6054 - val_accuracy: 0.7257\n",
            "Epoch 127/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6045 - accuracy: 0.7275 - val_loss: 0.6076 - val_accuracy: 0.7244\n",
            "Epoch 128/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6044 - accuracy: 0.7277 - val_loss: 0.6114 - val_accuracy: 0.7251\n",
            "Epoch 129/256\n",
            "15092/15092 [==============================] - 38s 2ms/step - loss: 0.6047 - accuracy: 0.7273 - val_loss: 0.6043 - val_accuracy: 0.7270\n",
            "Epoch 130/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6043 - accuracy: 0.7276 - val_loss: 0.6241 - val_accuracy: 0.7173\n",
            "Epoch 131/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6050 - accuracy: 0.7274 - val_loss: 0.6037 - val_accuracy: 0.7276\n",
            "Epoch 132/256\n",
            "15092/15092 [==============================] - 38s 2ms/step - loss: 0.6045 - accuracy: 0.7279 - val_loss: 0.6179 - val_accuracy: 0.7225\n",
            "Epoch 133/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.6038 - accuracy: 0.7277 - val_loss: 0.6093 - val_accuracy: 0.7197\n",
            "Epoch 134/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6045 - accuracy: 0.7278 - val_loss: 0.6053 - val_accuracy: 0.7228\n",
            "Epoch 135/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6028 - accuracy: 0.7278 - val_loss: 0.5998 - val_accuracy: 0.7275\n",
            "Epoch 136/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6030 - accuracy: 0.7279 - val_loss: 0.6194 - val_accuracy: 0.7209\n",
            "Epoch 137/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6027 - accuracy: 0.7282 - val_loss: 0.6082 - val_accuracy: 0.7210\n",
            "Epoch 138/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6033 - accuracy: 0.7282 - val_loss: 0.6159 - val_accuracy: 0.7241\n",
            "Epoch 139/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6025 - accuracy: 0.7285 - val_loss: 0.6090 - val_accuracy: 0.7220\n",
            "Epoch 140/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6049 - accuracy: 0.7277 - val_loss: 0.6102 - val_accuracy: 0.7207\n",
            "Epoch 141/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6019 - accuracy: 0.7284 - val_loss: 0.6010 - val_accuracy: 0.7209\n",
            "Epoch 142/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6011 - accuracy: 0.7297 - val_loss: 0.5960 - val_accuracy: 0.7307\n",
            "Epoch 143/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6041 - accuracy: 0.7280 - val_loss: 0.6192 - val_accuracy: 0.7222\n",
            "Epoch 144/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6025 - accuracy: 0.7285 - val_loss: 0.6337 - val_accuracy: 0.7073\n",
            "Epoch 145/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6015 - accuracy: 0.7291 - val_loss: 0.6011 - val_accuracy: 0.7264\n",
            "Epoch 146/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.6014 - accuracy: 0.7291 - val_loss: 0.6001 - val_accuracy: 0.7290\n",
            "Epoch 147/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.6008 - accuracy: 0.7293 - val_loss: 0.6200 - val_accuracy: 0.7174\n",
            "Epoch 148/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6013 - accuracy: 0.7297 - val_loss: 0.6030 - val_accuracy: 0.7289\n",
            "Epoch 149/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.6004 - accuracy: 0.7297 - val_loss: 0.6307 - val_accuracy: 0.7174\n",
            "Epoch 150/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.6004 - accuracy: 0.7296 - val_loss: 0.6054 - val_accuracy: 0.7246\n",
            "Epoch 151/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5993 - accuracy: 0.7303 - val_loss: 0.6082 - val_accuracy: 0.7241\n",
            "Epoch 152/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.5990 - accuracy: 0.7304 - val_loss: 0.6182 - val_accuracy: 0.7197\n",
            "Epoch 153/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.5992 - accuracy: 0.7298 - val_loss: 0.6030 - val_accuracy: 0.7267\n",
            "Epoch 154/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5989 - accuracy: 0.7303 - val_loss: 0.6148 - val_accuracy: 0.7251\n",
            "Epoch 155/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5999 - accuracy: 0.7303 - val_loss: 0.5946 - val_accuracy: 0.7303\n",
            "Epoch 156/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5990 - accuracy: 0.7301 - val_loss: 0.6113 - val_accuracy: 0.7248\n",
            "Epoch 157/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5996 - accuracy: 0.7302 - val_loss: 0.6089 - val_accuracy: 0.7225\n",
            "Epoch 158/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5983 - accuracy: 0.7309 - val_loss: 0.6214 - val_accuracy: 0.7171\n",
            "Epoch 159/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5982 - accuracy: 0.7306 - val_loss: 0.6003 - val_accuracy: 0.7233\n",
            "Epoch 160/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.5982 - accuracy: 0.7310 - val_loss: 0.5945 - val_accuracy: 0.7328\n",
            "Epoch 161/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5981 - accuracy: 0.7304 - val_loss: 0.5948 - val_accuracy: 0.7309\n",
            "Epoch 162/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5983 - accuracy: 0.7302 - val_loss: 0.6036 - val_accuracy: 0.7244\n",
            "Epoch 163/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5974 - accuracy: 0.7310 - val_loss: 0.6025 - val_accuracy: 0.7258\n",
            "Epoch 164/256\n",
            "15092/15092 [==============================] - 43s 3ms/step - loss: 0.5971 - accuracy: 0.7315 - val_loss: 0.6010 - val_accuracy: 0.7291\n",
            "Epoch 165/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5974 - accuracy: 0.7309 - val_loss: 0.5998 - val_accuracy: 0.7281\n",
            "Epoch 166/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5966 - accuracy: 0.7318 - val_loss: 0.6020 - val_accuracy: 0.7243\n",
            "Epoch 167/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5963 - accuracy: 0.7317 - val_loss: 0.5913 - val_accuracy: 0.7317\n",
            "Epoch 168/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.5958 - accuracy: 0.7314 - val_loss: 0.6071 - val_accuracy: 0.7269\n",
            "Epoch 169/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.5970 - accuracy: 0.7313 - val_loss: 0.6023 - val_accuracy: 0.7214\n",
            "Epoch 170/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5966 - accuracy: 0.7316 - val_loss: 0.6029 - val_accuracy: 0.7250\n",
            "Epoch 171/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.5962 - accuracy: 0.7317 - val_loss: 0.5992 - val_accuracy: 0.7333\n",
            "Epoch 172/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.5958 - accuracy: 0.7321 - val_loss: 0.5953 - val_accuracy: 0.7304\n",
            "Epoch 173/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5957 - accuracy: 0.7319 - val_loss: 0.6095 - val_accuracy: 0.7250\n",
            "Epoch 174/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5951 - accuracy: 0.7322 - val_loss: 0.6308 - val_accuracy: 0.7137\n",
            "Epoch 175/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5947 - accuracy: 0.7324 - val_loss: 0.6343 - val_accuracy: 0.7073\n",
            "Epoch 176/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.5951 - accuracy: 0.7326 - val_loss: 0.6015 - val_accuracy: 0.7254\n",
            "Epoch 177/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.5950 - accuracy: 0.7322 - val_loss: 0.6007 - val_accuracy: 0.7235\n",
            "Epoch 178/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5947 - accuracy: 0.7325 - val_loss: 0.6063 - val_accuracy: 0.7233\n",
            "Epoch 179/256\n",
            "15092/15092 [==============================] - 42s 3ms/step - loss: 0.5950 - accuracy: 0.7325 - val_loss: 0.6014 - val_accuracy: 0.7279\n",
            "Epoch 180/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5951 - accuracy: 0.7329 - val_loss: 0.5922 - val_accuracy: 0.7340\n",
            "Epoch 181/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5948 - accuracy: 0.7326 - val_loss: 0.6108 - val_accuracy: 0.7233\n",
            "Epoch 182/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5938 - accuracy: 0.7330 - val_loss: 0.6206 - val_accuracy: 0.7166\n",
            "Epoch 183/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5931 - accuracy: 0.7332 - val_loss: 0.6044 - val_accuracy: 0.7259\n",
            "Epoch 184/256\n",
            "15092/15092 [==============================] - 41s 3ms/step - loss: 0.5934 - accuracy: 0.7330 - val_loss: 0.6003 - val_accuracy: 0.7275\n",
            "Epoch 185/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5935 - accuracy: 0.7326 - val_loss: 0.6114 - val_accuracy: 0.7216\n",
            "Epoch 186/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5928 - accuracy: 0.7333 - val_loss: 0.6096 - val_accuracy: 0.7262\n",
            "Epoch 187/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5927 - accuracy: 0.7332 - val_loss: 0.6050 - val_accuracy: 0.7266\n",
            "Epoch 188/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5933 - accuracy: 0.7329 - val_loss: 0.6043 - val_accuracy: 0.7227\n",
            "Epoch 189/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5925 - accuracy: 0.7334 - val_loss: 0.6024 - val_accuracy: 0.7254\n",
            "Epoch 190/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5927 - accuracy: 0.7335 - val_loss: 0.5929 - val_accuracy: 0.7295\n",
            "Epoch 191/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5913 - accuracy: 0.7343 - val_loss: 0.5988 - val_accuracy: 0.7286\n",
            "Epoch 192/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.5922 - accuracy: 0.7338 - val_loss: 0.5970 - val_accuracy: 0.7248\n",
            "Epoch 193/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5912 - accuracy: 0.7342 - val_loss: 0.6314 - val_accuracy: 0.7131\n",
            "Epoch 194/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5921 - accuracy: 0.7338 - val_loss: 0.5898 - val_accuracy: 0.7320\n",
            "Epoch 195/256\n",
            "15092/15092 [==============================] - 40s 3ms/step - loss: 0.5920 - accuracy: 0.7335 - val_loss: 0.5987 - val_accuracy: 0.7296\n",
            "Epoch 196/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5914 - accuracy: 0.7340 - val_loss: 0.5958 - val_accuracy: 0.7317\n",
            "Epoch 197/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5923 - accuracy: 0.7335 - val_loss: 0.6059 - val_accuracy: 0.7248\n",
            "Epoch 198/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5912 - accuracy: 0.7342 - val_loss: 0.5950 - val_accuracy: 0.7340\n",
            "Epoch 199/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5901 - accuracy: 0.7347 - val_loss: 0.5992 - val_accuracy: 0.7293\n",
            "Epoch 200/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5903 - accuracy: 0.7344 - val_loss: 0.5961 - val_accuracy: 0.7300\n",
            "Epoch 201/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5905 - accuracy: 0.7343 - val_loss: 0.5887 - val_accuracy: 0.7296\n",
            "Epoch 202/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5910 - accuracy: 0.7346 - val_loss: 0.5920 - val_accuracy: 0.7291\n",
            "Epoch 203/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5899 - accuracy: 0.7350 - val_loss: 0.5894 - val_accuracy: 0.7324\n",
            "Epoch 204/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5900 - accuracy: 0.7349 - val_loss: 0.5958 - val_accuracy: 0.7306\n",
            "Epoch 205/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5902 - accuracy: 0.7348 - val_loss: 0.6055 - val_accuracy: 0.7206\n",
            "Epoch 206/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5905 - accuracy: 0.7345 - val_loss: 0.5902 - val_accuracy: 0.7290\n",
            "Epoch 207/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5897 - accuracy: 0.7350 - val_loss: 0.5985 - val_accuracy: 0.7299\n",
            "Epoch 208/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5892 - accuracy: 0.7354 - val_loss: 0.5990 - val_accuracy: 0.7267\n",
            "Epoch 209/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5895 - accuracy: 0.7354 - val_loss: 0.5941 - val_accuracy: 0.7331\n",
            "Epoch 210/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5897 - accuracy: 0.7347 - val_loss: 0.6067 - val_accuracy: 0.7231\n",
            "Epoch 211/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5893 - accuracy: 0.7346 - val_loss: 0.5998 - val_accuracy: 0.7307\n",
            "Epoch 212/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5890 - accuracy: 0.7350 - val_loss: 0.6039 - val_accuracy: 0.7262\n",
            "Epoch 213/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5894 - accuracy: 0.7352 - val_loss: 0.6027 - val_accuracy: 0.7268\n",
            "Epoch 214/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5893 - accuracy: 0.7348 - val_loss: 0.5975 - val_accuracy: 0.7297\n",
            "Epoch 215/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5897 - accuracy: 0.7352 - val_loss: 0.6038 - val_accuracy: 0.7254\n",
            "Epoch 216/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5885 - accuracy: 0.7353 - val_loss: 0.5871 - val_accuracy: 0.7377\n",
            "Epoch 217/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5880 - accuracy: 0.7359 - val_loss: 0.6196 - val_accuracy: 0.7147\n",
            "Epoch 218/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5890 - accuracy: 0.7350 - val_loss: 0.5869 - val_accuracy: 0.7313\n",
            "Epoch 219/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5877 - accuracy: 0.7360 - val_loss: 0.6443 - val_accuracy: 0.7050\n",
            "Epoch 220/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5897 - accuracy: 0.7351 - val_loss: 0.5927 - val_accuracy: 0.7323\n",
            "Epoch 221/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5888 - accuracy: 0.7351 - val_loss: 0.6007 - val_accuracy: 0.7299\n",
            "Epoch 222/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5885 - accuracy: 0.7355 - val_loss: 0.5941 - val_accuracy: 0.7276\n",
            "Epoch 223/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5872 - accuracy: 0.7361 - val_loss: 0.5869 - val_accuracy: 0.7340\n",
            "Epoch 224/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5873 - accuracy: 0.7360 - val_loss: 0.5927 - val_accuracy: 0.7327\n",
            "Epoch 225/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5877 - accuracy: 0.7358 - val_loss: 0.5818 - val_accuracy: 0.7407\n",
            "Epoch 226/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5874 - accuracy: 0.7358 - val_loss: 0.5886 - val_accuracy: 0.7341\n",
            "Epoch 227/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5876 - accuracy: 0.7359 - val_loss: 0.5850 - val_accuracy: 0.7324\n",
            "Epoch 228/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5874 - accuracy: 0.7360 - val_loss: 0.6158 - val_accuracy: 0.7191\n",
            "Epoch 229/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5886 - accuracy: 0.7355 - val_loss: 0.5878 - val_accuracy: 0.7319\n",
            "Epoch 230/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5875 - accuracy: 0.7363 - val_loss: 0.6046 - val_accuracy: 0.7264\n",
            "Epoch 231/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5865 - accuracy: 0.7359 - val_loss: 0.6097 - val_accuracy: 0.7258\n",
            "Epoch 232/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5871 - accuracy: 0.7365 - val_loss: 0.6100 - val_accuracy: 0.7215\n",
            "Epoch 233/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5864 - accuracy: 0.7364 - val_loss: 0.5941 - val_accuracy: 0.7297\n",
            "Epoch 234/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5864 - accuracy: 0.7364 - val_loss: 0.5787 - val_accuracy: 0.7361\n",
            "Epoch 235/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5869 - accuracy: 0.7360 - val_loss: 0.6113 - val_accuracy: 0.7242\n",
            "Epoch 236/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5862 - accuracy: 0.7366 - val_loss: 0.5983 - val_accuracy: 0.7277\n",
            "Epoch 237/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5866 - accuracy: 0.7365 - val_loss: 0.5799 - val_accuracy: 0.7368\n",
            "Epoch 238/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5868 - accuracy: 0.7362 - val_loss: 0.5943 - val_accuracy: 0.7289\n",
            "Epoch 239/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5861 - accuracy: 0.7362 - val_loss: 0.5820 - val_accuracy: 0.7320\n",
            "Epoch 240/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5867 - accuracy: 0.7362 - val_loss: 0.6238 - val_accuracy: 0.7143\n",
            "Epoch 241/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5864 - accuracy: 0.7364 - val_loss: 0.5864 - val_accuracy: 0.7330\n",
            "Epoch 242/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5863 - accuracy: 0.7366 - val_loss: 0.6007 - val_accuracy: 0.7291\n",
            "Epoch 243/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5868 - accuracy: 0.7367 - val_loss: 0.5821 - val_accuracy: 0.7384\n",
            "Epoch 244/256\n",
            "15092/15092 [==============================] - 38s 3ms/step - loss: 0.5867 - accuracy: 0.7365 - val_loss: 0.6075 - val_accuracy: 0.7261\n",
            "Epoch 245/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5860 - accuracy: 0.7365 - val_loss: 0.5828 - val_accuracy: 0.7379\n",
            "Epoch 246/256\n",
            "15092/15092 [==============================] - 39s 3ms/step - loss: 0.5864 - accuracy: 0.7362 - val_loss: 0.5959 - val_accuracy: 0.7309\n",
            "Epoch 247/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5852 - accuracy: 0.7372 - val_loss: 0.6019 - val_accuracy: 0.7247\n",
            "Epoch 248/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5860 - accuracy: 0.7363 - val_loss: 0.5877 - val_accuracy: 0.7313\n",
            "Epoch 249/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5857 - accuracy: 0.7371 - val_loss: 0.5996 - val_accuracy: 0.7259\n",
            "Epoch 250/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5847 - accuracy: 0.7367 - val_loss: 0.5986 - val_accuracy: 0.7325\n",
            "Epoch 251/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5854 - accuracy: 0.7370 - val_loss: 0.5839 - val_accuracy: 0.7332\n",
            "Epoch 252/256\n",
            "15092/15092 [==============================] - 37s 2ms/step - loss: 0.5849 - accuracy: 0.7374 - val_loss: 0.5834 - val_accuracy: 0.7373\n",
            "Epoch 253/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5855 - accuracy: 0.7371 - val_loss: 0.5840 - val_accuracy: 0.7387\n",
            "Epoch 254/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5861 - accuracy: 0.7368 - val_loss: 0.5985 - val_accuracy: 0.7253\n",
            "Epoch 255/256\n",
            "15092/15092 [==============================] - 36s 2ms/step - loss: 0.5848 - accuracy: 0.7373 - val_loss: 0.5901 - val_accuracy: 0.7336\n",
            "Epoch 256/256\n",
            "15092/15092 [==============================] - 35s 2ms/step - loss: 0.5841 - accuracy: 0.7374 - val_loss: 0.5980 - val_accuracy: 0.7254\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              multiple                  1280      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              multiple                  387       \n",
            "=================================================================\n",
            "Total params: 51,203\n",
            "Trainable params: 51,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Evaluate on test data\n",
            "3268/3268 [==============================] - 5s 1ms/step - loss: 0.5962 - accuracy: 0.7307\n",
            "test loss, test acc: [0.5962150692939758, 0.7307416200637817]\n",
            "Generate predictions for 3 samples\n",
            "predictions shape: (3, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}