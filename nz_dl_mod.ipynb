{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nz_dl_mod.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTogr/VOZRNlsqbzNCT3kj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/widnu/Predictive_Policing_DL/blob/master/nz_dl_mod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74owbDLPHimY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eaa0939f-f24a-48c6-81a0-42323f8063bb"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DLWazIzycWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "#df=pd.read_csv('gdrive/My Drive/Colab Notebooks/dataset/nz_crime_dataset_14151617.csv', encoding='utf-8-sig')*\n",
        "df=pd.read_csv('gdrive/My Drive/Colab Notebooks/dataset/nz_crime_dataset.csv', encoding='utf-8-sig')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTYiXGH0Txt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "01ab4caa-b0c7-436a-aa23-074713c69cba"
      },
      "source": [
        "df.info()\n",
        "df.describe()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1167347 entries, 0 to 1167346\n",
            "Data columns (total 17 columns):\n",
            " #   Column                 Non-Null Count    Dtype  \n",
            "---  ------                 --------------    -----  \n",
            " 0   DATE_NO_TIME           1167347 non-null  object \n",
            " 1   DATE_TIME              1167347 non-null  object \n",
            " 2   7_DAYS_CRIME           1167347 non-null  float64\n",
            " 3   1_MONTH_CRIME          1167347 non-null  float64\n",
            " 4   1_YEAR_CRIME           1167347 non-null  float64\n",
            " 5   TIME_SINCE_LAST_CRIME  1167347 non-null  float64\n",
            " 6   MONTH                  1167347 non-null  int64  \n",
            " 7   QUARTER                1167347 non-null  float64\n",
            " 8   DAY_OF_WEEK            1167347 non-null  object \n",
            " 9   DAY                    1167347 non-null  float64\n",
            " 10  HOUR                   1167347 non-null  float64\n",
            " 11  HOUR_PARTITION         1167347 non-null  float64\n",
            " 12  MESHBLOCK              1167347 non-null  int64  \n",
            " 13  AREA_0                 1167347 non-null  object \n",
            " 14  AREA_1                 1167347 non-null  object \n",
            " 15  WEAPON_TYPE            1167347 non-null  int64  \n",
            " 16  CRIME_TYPE             1167347 non-null  object \n",
            "dtypes: float64(8), int64(3), object(6)\n",
            "memory usage: 151.4+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>7_DAYS_CRIME</th>\n",
              "      <th>1_MONTH_CRIME</th>\n",
              "      <th>1_YEAR_CRIME</th>\n",
              "      <th>TIME_SINCE_LAST_CRIME</th>\n",
              "      <th>MONTH</th>\n",
              "      <th>QUARTER</th>\n",
              "      <th>DAY</th>\n",
              "      <th>HOUR</th>\n",
              "      <th>HOUR_PARTITION</th>\n",
              "      <th>MESHBLOCK</th>\n",
              "      <th>WEAPON_TYPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "      <td>1.167347e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8.766996e+00</td>\n",
              "      <td>9.575736e+00</td>\n",
              "      <td>1.205730e+01</td>\n",
              "      <td>1.281016e-01</td>\n",
              "      <td>6.536036e+00</td>\n",
              "      <td>2.512512e+00</td>\n",
              "      <td>5.013381e+00</td>\n",
              "      <td>1.310084e+01</td>\n",
              "      <td>1.205440e+01</td>\n",
              "      <td>1.302863e+06</td>\n",
              "      <td>9.731468e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000510e+00</td>\n",
              "      <td>2.638901e-01</td>\n",
              "      <td>5.601060e-01</td>\n",
              "      <td>7.455829e+00</td>\n",
              "      <td>3.552330e+00</td>\n",
              "      <td>1.149184e+00</td>\n",
              "      <td>2.150864e+00</td>\n",
              "      <td>4.183061e+00</td>\n",
              "      <td>4.167649e+00</td>\n",
              "      <td>8.577749e+05</td>\n",
              "      <td>1.042445e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-1.461000e+03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.377701e+00</td>\n",
              "      <td>9.481893e+00</td>\n",
              "      <td>1.217237e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>1.262178e+01</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>6.075000e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.071538e+00</td>\n",
              "      <td>9.612667e+00</td>\n",
              "      <td>1.222303e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>6.000000e+00</td>\n",
              "      <td>1.317855e+01</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>1.167000e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.474011e+00</td>\n",
              "      <td>9.706986e+00</td>\n",
              "      <td>1.224577e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>1.400000e+01</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>2.029200e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000983e+01</td>\n",
              "      <td>1.023002e+01</td>\n",
              "      <td>1.235463e+01</td>\n",
              "      <td>5.770000e+02</td>\n",
              "      <td>1.200000e+01</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>2.300000e+01</td>\n",
              "      <td>2.100000e+01</td>\n",
              "      <td>3.210003e+06</td>\n",
              "      <td>2.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       7_DAYS_CRIME  1_MONTH_CRIME  ...     MESHBLOCK   WEAPON_TYPE\n",
              "count  1.167347e+06   1.167347e+06  ...  1.167347e+06  1.167347e+06\n",
              "mean   8.766996e+00   9.575736e+00  ...  1.302863e+06  9.731468e-03\n",
              "std    1.000510e+00   2.638901e-01  ...  8.577749e+05  1.042445e-01\n",
              "min    0.000000e+00   0.000000e+00  ... -1.461000e+03  0.000000e+00\n",
              "25%    8.377701e+00   9.481893e+00  ...  6.075000e+05  0.000000e+00\n",
              "50%    9.071538e+00   9.612667e+00  ...  1.167000e+06  0.000000e+00\n",
              "75%    9.474011e+00   9.706986e+00  ...  2.029200e+06  0.000000e+00\n",
              "max    1.000983e+01   1.023002e+01  ...  3.210003e+06  2.000000e+00\n",
              "\n",
              "[8 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FVtG3Ia1e09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "c418d91d-cbd5-4fae-c584-5d80b38f87f4"
      },
      "source": [
        "#######################################################\n",
        "# Count total NaN at each column in DataFrame\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Count all NaN in a DataFrame (both columns & Rows)\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "# erase every row (axis=0) that has \"any\" Null value in it.\n",
        "df = df.dropna(how='any',axis=0) "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATE_NO_TIME             0\n",
            "DATE_TIME                0\n",
            "7_DAYS_CRIME             0\n",
            "1_MONTH_CRIME            0\n",
            "1_YEAR_CRIME             0\n",
            "TIME_SINCE_LAST_CRIME    0\n",
            "MONTH                    0\n",
            "QUARTER                  0\n",
            "DAY_OF_WEEK              0\n",
            "DAY                      0\n",
            "HOUR                     0\n",
            "HOUR_PARTITION           0\n",
            "MESHBLOCK                0\n",
            "AREA_0                   0\n",
            "AREA_1                   0\n",
            "WEAPON_TYPE              0\n",
            "CRIME_TYPE               0\n",
            "dtype: int64\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAzMTKyP1kOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################\n",
        "feature_var = ['7_DAYS_CRIME', '1_MONTH_CRIME', '1_YEAR_CRIME', 'TIME_SINCE_LAST_CRIME', 'MONTH', 'DAY', 'QUARTER', 'HOUR_PARTITION', 'AREA_0', 'AREA_1', 'WEAPON_TYPE']\n",
        "# feature_var = ['AREA_0', 'AREA_1']\n",
        "response_var = 'CRIME_TYPE'\n",
        "#######################################################\n",
        "X = df[feature_var]\n",
        "y = df.pop(response_var)\n",
        "# y = df[response_var]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWbzwzPw1sYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "ecc8ab9d-23e0-4548-eb49-031da822ccf6"
      },
      "source": [
        "########################################\n",
        "# Encode categorical variables\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oe = OrdinalEncoder()\n",
        "oe.fit(X)\n",
        "X = oe.transform(X)\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA(n_components=5)\n",
        "# X = pca.fit_transform(X)\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc = StandardScaler()\n",
        "# X = sc.fit_transform(X)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "y = le.transform(y)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# from keras.utils import to_categorical\n",
        "# y = to_categorical(y)\n",
        "\n",
        "#######################################################\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "# transform the dataset\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y)\n",
        "print(counter)\n",
        "\n",
        "#######################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 5)\n",
        "\n",
        "# Reserve 10,000 samples for validation\n",
        "X_val = X_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "X_train = X_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "#######################################################\n",
        "#df_X = pd.DataFrame(data=X)\n",
        "#df_y = pd.DataFrame(data=y)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1: 676408, 2: 371126, 0: 119813})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Counter({1: 676408, 2: 676408, 0: 676408})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNmYLYTX1yr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "#dataset = tf.data.Dataset.from_tensor_slices((df_X.values, df_y.values))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6GTytTP2Va0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_dataset = dataset.shuffle(len(df_X)).batch(1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMcBPgWI2OQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_compiled_model():\n",
        "    model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6P7I7Nx2mcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9dde487f-b675-41d3-b9bf-116c4180b028"
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=256,\n",
        "    validation_data=(X_val, y_val),\n",
        ")\n",
        "\n",
        "history.history\n",
        "model.summary()\n",
        "\n",
        "# https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/\n",
        "\n",
        "##################################################\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(X_test, y_test, batch_size=128)\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "# Generate predictions (probabilities -- the output of the last layer)\n",
        "# on new data using `predict`\n",
        "print(\"Generate predictions for 3 samples\")\n",
        "predictions = model.predict(X_test[:3])\n",
        "print(\"predictions shape:\", predictions.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.9220 - accuracy: 0.5440 - val_loss: 0.9116 - val_accuracy: 0.5485\n",
            "Epoch 2/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.8974 - accuracy: 0.5593 - val_loss: 0.8873 - val_accuracy: 0.5663\n",
            "Epoch 3/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.8809 - accuracy: 0.5712 - val_loss: 0.8744 - val_accuracy: 0.5796\n",
            "Epoch 4/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.8602 - accuracy: 0.5862 - val_loss: 0.8563 - val_accuracy: 0.5946\n",
            "Epoch 5/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.8402 - accuracy: 0.6009 - val_loss: 0.8354 - val_accuracy: 0.6087\n",
            "Epoch 6/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.8229 - accuracy: 0.6125 - val_loss: 0.8159 - val_accuracy: 0.6196\n",
            "Epoch 7/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.8084 - accuracy: 0.6221 - val_loss: 0.8136 - val_accuracy: 0.6240\n",
            "Epoch 8/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.7969 - accuracy: 0.6299 - val_loss: 0.7886 - val_accuracy: 0.6368\n",
            "Epoch 9/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.7864 - accuracy: 0.6366 - val_loss: 0.7878 - val_accuracy: 0.6387\n",
            "Epoch 10/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.7777 - accuracy: 0.6421 - val_loss: 0.7693 - val_accuracy: 0.6499\n",
            "Epoch 11/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.7699 - accuracy: 0.6468 - val_loss: 0.7723 - val_accuracy: 0.6503\n",
            "Epoch 12/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.7630 - accuracy: 0.6515 - val_loss: 0.7601 - val_accuracy: 0.6531\n",
            "Epoch 13/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.7566 - accuracy: 0.6552 - val_loss: 0.7730 - val_accuracy: 0.6534\n",
            "Epoch 14/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.7509 - accuracy: 0.6582 - val_loss: 0.7541 - val_accuracy: 0.6596\n",
            "Epoch 15/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.7457 - accuracy: 0.6617 - val_loss: 0.7558 - val_accuracy: 0.6572\n",
            "Epoch 16/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7403 - accuracy: 0.6651 - val_loss: 0.7438 - val_accuracy: 0.6617\n",
            "Epoch 17/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7358 - accuracy: 0.6678 - val_loss: 0.7322 - val_accuracy: 0.6730\n",
            "Epoch 18/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7315 - accuracy: 0.6707 - val_loss: 0.7370 - val_accuracy: 0.6683\n",
            "Epoch 19/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.7271 - accuracy: 0.6733 - val_loss: 0.7220 - val_accuracy: 0.6781\n",
            "Epoch 20/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.7231 - accuracy: 0.6758 - val_loss: 0.7230 - val_accuracy: 0.6767\n",
            "Epoch 21/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7193 - accuracy: 0.6784 - val_loss: 0.7180 - val_accuracy: 0.6780\n",
            "Epoch 22/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7161 - accuracy: 0.6797 - val_loss: 0.7416 - val_accuracy: 0.6711\n",
            "Epoch 23/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7125 - accuracy: 0.6824 - val_loss: 0.7287 - val_accuracy: 0.6774\n",
            "Epoch 24/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7099 - accuracy: 0.6838 - val_loss: 0.7116 - val_accuracy: 0.6846\n",
            "Epoch 25/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.7072 - accuracy: 0.6855 - val_loss: 0.7244 - val_accuracy: 0.6767\n",
            "Epoch 26/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7044 - accuracy: 0.6872 - val_loss: 0.7150 - val_accuracy: 0.6844\n",
            "Epoch 27/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.7016 - accuracy: 0.6891 - val_loss: 0.7178 - val_accuracy: 0.6822\n",
            "Epoch 28/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6995 - accuracy: 0.6905 - val_loss: 0.7210 - val_accuracy: 0.6828\n",
            "Epoch 29/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6970 - accuracy: 0.6921 - val_loss: 0.7007 - val_accuracy: 0.6955\n",
            "Epoch 30/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6949 - accuracy: 0.6932 - val_loss: 0.7022 - val_accuracy: 0.6930\n",
            "Epoch 31/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6922 - accuracy: 0.6948 - val_loss: 0.6917 - val_accuracy: 0.6988\n",
            "Epoch 32/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6898 - accuracy: 0.6962 - val_loss: 0.6973 - val_accuracy: 0.6913\n",
            "Epoch 33/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6878 - accuracy: 0.6974 - val_loss: 0.6984 - val_accuracy: 0.6966\n",
            "Epoch 34/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6854 - accuracy: 0.6991 - val_loss: 0.6898 - val_accuracy: 0.7007\n",
            "Epoch 35/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6838 - accuracy: 0.7002 - val_loss: 0.6915 - val_accuracy: 0.6990\n",
            "Epoch 36/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6814 - accuracy: 0.7014 - val_loss: 0.6822 - val_accuracy: 0.7026\n",
            "Epoch 37/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6796 - accuracy: 0.7019 - val_loss: 0.6898 - val_accuracy: 0.7013\n",
            "Epoch 38/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6781 - accuracy: 0.7029 - val_loss: 0.6824 - val_accuracy: 0.7049\n",
            "Epoch 39/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6762 - accuracy: 0.7042 - val_loss: 0.6802 - val_accuracy: 0.7038\n",
            "Epoch 40/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6744 - accuracy: 0.7050 - val_loss: 0.6907 - val_accuracy: 0.6995\n",
            "Epoch 41/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6727 - accuracy: 0.7059 - val_loss: 0.6862 - val_accuracy: 0.7056\n",
            "Epoch 42/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6716 - accuracy: 0.7065 - val_loss: 0.6805 - val_accuracy: 0.7060\n",
            "Epoch 43/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6699 - accuracy: 0.7075 - val_loss: 0.6814 - val_accuracy: 0.7030\n",
            "Epoch 44/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6684 - accuracy: 0.7086 - val_loss: 0.6852 - val_accuracy: 0.7022\n",
            "Epoch 45/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6670 - accuracy: 0.7090 - val_loss: 0.6686 - val_accuracy: 0.7076\n",
            "Epoch 46/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6656 - accuracy: 0.7095 - val_loss: 0.6808 - val_accuracy: 0.7029\n",
            "Epoch 47/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6646 - accuracy: 0.7107 - val_loss: 0.6643 - val_accuracy: 0.7138\n",
            "Epoch 48/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6635 - accuracy: 0.7113 - val_loss: 0.6801 - val_accuracy: 0.7088\n",
            "Epoch 49/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6626 - accuracy: 0.7117 - val_loss: 0.6815 - val_accuracy: 0.7049\n",
            "Epoch 50/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6610 - accuracy: 0.7125 - val_loss: 0.6694 - val_accuracy: 0.7116\n",
            "Epoch 51/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6598 - accuracy: 0.7129 - val_loss: 0.6660 - val_accuracy: 0.7142\n",
            "Epoch 52/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6587 - accuracy: 0.7135 - val_loss: 0.6720 - val_accuracy: 0.7078\n",
            "Epoch 53/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6579 - accuracy: 0.7142 - val_loss: 0.6721 - val_accuracy: 0.7095\n",
            "Epoch 54/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6563 - accuracy: 0.7155 - val_loss: 0.6679 - val_accuracy: 0.7107\n",
            "Epoch 55/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6556 - accuracy: 0.7157 - val_loss: 0.6589 - val_accuracy: 0.7153\n",
            "Epoch 56/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6549 - accuracy: 0.7160 - val_loss: 0.6570 - val_accuracy: 0.7201\n",
            "Epoch 57/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6538 - accuracy: 0.7166 - val_loss: 0.6778 - val_accuracy: 0.7094\n",
            "Epoch 58/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6525 - accuracy: 0.7173 - val_loss: 0.6702 - val_accuracy: 0.7142\n",
            "Epoch 59/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6518 - accuracy: 0.7177 - val_loss: 0.6656 - val_accuracy: 0.7135\n",
            "Epoch 60/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6508 - accuracy: 0.7180 - val_loss: 0.6565 - val_accuracy: 0.7140\n",
            "Epoch 61/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6504 - accuracy: 0.7187 - val_loss: 0.6735 - val_accuracy: 0.7085\n",
            "Epoch 62/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6491 - accuracy: 0.7190 - val_loss: 0.6497 - val_accuracy: 0.7201\n",
            "Epoch 63/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6486 - accuracy: 0.7194 - val_loss: 0.6672 - val_accuracy: 0.7133\n",
            "Epoch 64/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6478 - accuracy: 0.7202 - val_loss: 0.6538 - val_accuracy: 0.7185\n",
            "Epoch 65/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6472 - accuracy: 0.7201 - val_loss: 0.6596 - val_accuracy: 0.7210\n",
            "Epoch 66/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6470 - accuracy: 0.7206 - val_loss: 0.6596 - val_accuracy: 0.7182\n",
            "Epoch 67/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6463 - accuracy: 0.7208 - val_loss: 0.6521 - val_accuracy: 0.7194\n",
            "Epoch 68/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6456 - accuracy: 0.7212 - val_loss: 0.6605 - val_accuracy: 0.7159\n",
            "Epoch 69/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6448 - accuracy: 0.7221 - val_loss: 0.6624 - val_accuracy: 0.7118\n",
            "Epoch 70/256\n",
            "22039/22039 [==============================] - 53s 2ms/step - loss: 0.6440 - accuracy: 0.7222 - val_loss: 0.6436 - val_accuracy: 0.7254\n",
            "Epoch 71/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6433 - accuracy: 0.7223 - val_loss: 0.6470 - val_accuracy: 0.7233\n",
            "Epoch 72/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6429 - accuracy: 0.7230 - val_loss: 0.6436 - val_accuracy: 0.7262\n",
            "Epoch 73/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6425 - accuracy: 0.7231 - val_loss: 0.6649 - val_accuracy: 0.7139\n",
            "Epoch 74/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6413 - accuracy: 0.7239 - val_loss: 0.6481 - val_accuracy: 0.7219\n",
            "Epoch 75/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6406 - accuracy: 0.7244 - val_loss: 0.6412 - val_accuracy: 0.7263\n",
            "Epoch 76/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6401 - accuracy: 0.7246 - val_loss: 0.6447 - val_accuracy: 0.7259\n",
            "Epoch 77/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6402 - accuracy: 0.7249 - val_loss: 0.6706 - val_accuracy: 0.7128\n",
            "Epoch 78/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6392 - accuracy: 0.7253 - val_loss: 0.6427 - val_accuracy: 0.7255\n",
            "Epoch 79/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6392 - accuracy: 0.7251 - val_loss: 0.6421 - val_accuracy: 0.7267\n",
            "Epoch 80/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6387 - accuracy: 0.7253 - val_loss: 0.6411 - val_accuracy: 0.7261\n",
            "Epoch 81/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6378 - accuracy: 0.7260 - val_loss: 0.6516 - val_accuracy: 0.7233\n",
            "Epoch 82/256\n",
            "22039/22039 [==============================] - 51s 2ms/step - loss: 0.6374 - accuracy: 0.7263 - val_loss: 0.6490 - val_accuracy: 0.7226\n",
            "Epoch 83/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6366 - accuracy: 0.7267 - val_loss: 0.6578 - val_accuracy: 0.7197\n",
            "Epoch 84/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6363 - accuracy: 0.7266 - val_loss: 0.6511 - val_accuracy: 0.7261\n",
            "Epoch 85/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6359 - accuracy: 0.7272 - val_loss: 0.6449 - val_accuracy: 0.7271\n",
            "Epoch 86/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6357 - accuracy: 0.7272 - val_loss: 0.6539 - val_accuracy: 0.7205\n",
            "Epoch 87/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6350 - accuracy: 0.7276 - val_loss: 0.6367 - val_accuracy: 0.7283\n",
            "Epoch 88/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6340 - accuracy: 0.7282 - val_loss: 0.6463 - val_accuracy: 0.7285\n",
            "Epoch 89/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6334 - accuracy: 0.7287 - val_loss: 0.6466 - val_accuracy: 0.7195\n",
            "Epoch 90/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6331 - accuracy: 0.7287 - val_loss: 0.6538 - val_accuracy: 0.7176\n",
            "Epoch 91/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6330 - accuracy: 0.7290 - val_loss: 0.6514 - val_accuracy: 0.7240\n",
            "Epoch 92/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6326 - accuracy: 0.7292 - val_loss: 0.6566 - val_accuracy: 0.7239\n",
            "Epoch 93/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6322 - accuracy: 0.7290 - val_loss: 0.6465 - val_accuracy: 0.7254\n",
            "Epoch 94/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6317 - accuracy: 0.7296 - val_loss: 0.6518 - val_accuracy: 0.7226\n",
            "Epoch 95/256\n",
            "22039/22039 [==============================] - 51s 2ms/step - loss: 0.6311 - accuracy: 0.7301 - val_loss: 0.6391 - val_accuracy: 0.7310\n",
            "Epoch 96/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6308 - accuracy: 0.7300 - val_loss: 0.6423 - val_accuracy: 0.7299\n",
            "Epoch 97/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6305 - accuracy: 0.7303 - val_loss: 0.6440 - val_accuracy: 0.7277\n",
            "Epoch 98/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6303 - accuracy: 0.7306 - val_loss: 0.6410 - val_accuracy: 0.7325\n",
            "Epoch 99/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6299 - accuracy: 0.7305 - val_loss: 0.6469 - val_accuracy: 0.7228\n",
            "Epoch 100/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6296 - accuracy: 0.7310 - val_loss: 0.6307 - val_accuracy: 0.7349\n",
            "Epoch 101/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6286 - accuracy: 0.7312 - val_loss: 0.6535 - val_accuracy: 0.7241\n",
            "Epoch 102/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6286 - accuracy: 0.7314 - val_loss: 0.6459 - val_accuracy: 0.7247\n",
            "Epoch 103/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6283 - accuracy: 0.7317 - val_loss: 0.6365 - val_accuracy: 0.7290\n",
            "Epoch 104/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6281 - accuracy: 0.7318 - val_loss: 0.6297 - val_accuracy: 0.7309\n",
            "Epoch 105/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6273 - accuracy: 0.7323 - val_loss: 0.6433 - val_accuracy: 0.7274\n",
            "Epoch 106/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6274 - accuracy: 0.7318 - val_loss: 0.6466 - val_accuracy: 0.7240\n",
            "Epoch 107/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6269 - accuracy: 0.7324 - val_loss: 0.6405 - val_accuracy: 0.7280\n",
            "Epoch 108/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6269 - accuracy: 0.7323 - val_loss: 0.6469 - val_accuracy: 0.7238\n",
            "Epoch 109/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6260 - accuracy: 0.7331 - val_loss: 0.6387 - val_accuracy: 0.7298\n",
            "Epoch 110/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6258 - accuracy: 0.7330 - val_loss: 0.6493 - val_accuracy: 0.7230\n",
            "Epoch 111/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6255 - accuracy: 0.7333 - val_loss: 0.6334 - val_accuracy: 0.7345\n",
            "Epoch 112/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6253 - accuracy: 0.7331 - val_loss: 0.6433 - val_accuracy: 0.7303\n",
            "Epoch 113/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6252 - accuracy: 0.7331 - val_loss: 0.6442 - val_accuracy: 0.7276\n",
            "Epoch 114/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6244 - accuracy: 0.7338 - val_loss: 0.6420 - val_accuracy: 0.7254\n",
            "Epoch 115/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6250 - accuracy: 0.7337 - val_loss: 0.6442 - val_accuracy: 0.7268\n",
            "Epoch 116/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6242 - accuracy: 0.7339 - val_loss: 0.6319 - val_accuracy: 0.7367\n",
            "Epoch 117/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6236 - accuracy: 0.7344 - val_loss: 0.6374 - val_accuracy: 0.7291\n",
            "Epoch 118/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6238 - accuracy: 0.7344 - val_loss: 0.6429 - val_accuracy: 0.7307\n",
            "Epoch 119/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6238 - accuracy: 0.7344 - val_loss: 0.6370 - val_accuracy: 0.7319\n",
            "Epoch 120/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6224 - accuracy: 0.7348 - val_loss: 0.6260 - val_accuracy: 0.7327\n",
            "Epoch 121/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6226 - accuracy: 0.7347 - val_loss: 0.6354 - val_accuracy: 0.7269\n",
            "Epoch 122/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6225 - accuracy: 0.7349 - val_loss: 0.6345 - val_accuracy: 0.7319\n",
            "Epoch 123/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6233 - accuracy: 0.7349 - val_loss: 0.6431 - val_accuracy: 0.7324\n",
            "Epoch 124/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6221 - accuracy: 0.7355 - val_loss: 0.6354 - val_accuracy: 0.7295\n",
            "Epoch 125/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6210 - accuracy: 0.7357 - val_loss: 0.6422 - val_accuracy: 0.7301\n",
            "Epoch 126/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6211 - accuracy: 0.7358 - val_loss: 0.6349 - val_accuracy: 0.7292\n",
            "Epoch 127/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6211 - accuracy: 0.7359 - val_loss: 0.6335 - val_accuracy: 0.7309\n",
            "Epoch 128/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6206 - accuracy: 0.7363 - val_loss: 0.6295 - val_accuracy: 0.7346\n",
            "Epoch 129/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6208 - accuracy: 0.7363 - val_loss: 0.6455 - val_accuracy: 0.7257\n",
            "Epoch 130/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6205 - accuracy: 0.7363 - val_loss: 0.6359 - val_accuracy: 0.7294\n",
            "Epoch 131/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6201 - accuracy: 0.7366 - val_loss: 0.6483 - val_accuracy: 0.7257\n",
            "Epoch 132/256\n",
            "22039/22039 [==============================] - 53s 2ms/step - loss: 0.6198 - accuracy: 0.7366 - val_loss: 0.6356 - val_accuracy: 0.7345\n",
            "Epoch 133/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6196 - accuracy: 0.7369 - val_loss: 0.6347 - val_accuracy: 0.7335\n",
            "Epoch 134/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6194 - accuracy: 0.7368 - val_loss: 0.6477 - val_accuracy: 0.7298\n",
            "Epoch 135/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6191 - accuracy: 0.7368 - val_loss: 0.6424 - val_accuracy: 0.7296\n",
            "Epoch 136/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6189 - accuracy: 0.7372 - val_loss: 0.6507 - val_accuracy: 0.7255\n",
            "Epoch 137/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6185 - accuracy: 0.7373 - val_loss: 0.6232 - val_accuracy: 0.7397\n",
            "Epoch 138/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6191 - accuracy: 0.7369 - val_loss: 0.6256 - val_accuracy: 0.7333\n",
            "Epoch 139/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6182 - accuracy: 0.7377 - val_loss: 0.6401 - val_accuracy: 0.7253\n",
            "Epoch 140/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6182 - accuracy: 0.7379 - val_loss: 0.6276 - val_accuracy: 0.7372\n",
            "Epoch 141/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6183 - accuracy: 0.7377 - val_loss: 0.6245 - val_accuracy: 0.7310\n",
            "Epoch 142/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6176 - accuracy: 0.7379 - val_loss: 0.6245 - val_accuracy: 0.7372\n",
            "Epoch 143/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6175 - accuracy: 0.7380 - val_loss: 0.6461 - val_accuracy: 0.7212\n",
            "Epoch 144/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6169 - accuracy: 0.7384 - val_loss: 0.6414 - val_accuracy: 0.7289\n",
            "Epoch 145/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6174 - accuracy: 0.7381 - val_loss: 0.6399 - val_accuracy: 0.7235\n",
            "Epoch 146/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6168 - accuracy: 0.7383 - val_loss: 0.6181 - val_accuracy: 0.7352\n",
            "Epoch 147/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6170 - accuracy: 0.7380 - val_loss: 0.6233 - val_accuracy: 0.7353\n",
            "Epoch 148/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6168 - accuracy: 0.7382 - val_loss: 0.6288 - val_accuracy: 0.7378\n",
            "Epoch 149/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6159 - accuracy: 0.7392 - val_loss: 0.6234 - val_accuracy: 0.7360\n",
            "Epoch 150/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6161 - accuracy: 0.7387 - val_loss: 0.6196 - val_accuracy: 0.7397\n",
            "Epoch 151/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6159 - accuracy: 0.7388 - val_loss: 0.6204 - val_accuracy: 0.7334\n",
            "Epoch 152/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6156 - accuracy: 0.7393 - val_loss: 0.6268 - val_accuracy: 0.7346\n",
            "Epoch 153/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6157 - accuracy: 0.7390 - val_loss: 0.6403 - val_accuracy: 0.7316\n",
            "Epoch 154/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6159 - accuracy: 0.7390 - val_loss: 0.6254 - val_accuracy: 0.7330\n",
            "Epoch 155/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6150 - accuracy: 0.7395 - val_loss: 0.6394 - val_accuracy: 0.7256\n",
            "Epoch 156/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6149 - accuracy: 0.7391 - val_loss: 0.6150 - val_accuracy: 0.7454\n",
            "Epoch 157/256\n",
            "22039/22039 [==============================] - 51s 2ms/step - loss: 0.6148 - accuracy: 0.7392 - val_loss: 0.6333 - val_accuracy: 0.7338\n",
            "Epoch 158/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6152 - accuracy: 0.7393 - val_loss: 0.6336 - val_accuracy: 0.7325\n",
            "Epoch 159/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6149 - accuracy: 0.7398 - val_loss: 0.6337 - val_accuracy: 0.7313\n",
            "Epoch 160/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6145 - accuracy: 0.7399 - val_loss: 0.6193 - val_accuracy: 0.7398\n",
            "Epoch 161/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6143 - accuracy: 0.7396 - val_loss: 0.6311 - val_accuracy: 0.7362\n",
            "Epoch 162/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6144 - accuracy: 0.7397 - val_loss: 0.6349 - val_accuracy: 0.7306\n",
            "Epoch 163/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6142 - accuracy: 0.7400 - val_loss: 0.6231 - val_accuracy: 0.7388\n",
            "Epoch 164/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6138 - accuracy: 0.7400 - val_loss: 0.6217 - val_accuracy: 0.7386\n",
            "Epoch 165/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6137 - accuracy: 0.7401 - val_loss: 0.6194 - val_accuracy: 0.7347\n",
            "Epoch 166/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6136 - accuracy: 0.7405 - val_loss: 0.6235 - val_accuracy: 0.7372\n",
            "Epoch 167/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6141 - accuracy: 0.7398 - val_loss: 0.6232 - val_accuracy: 0.7391\n",
            "Epoch 168/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6135 - accuracy: 0.7402 - val_loss: 0.6348 - val_accuracy: 0.7317\n",
            "Epoch 169/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6130 - accuracy: 0.7404 - val_loss: 0.6217 - val_accuracy: 0.7338\n",
            "Epoch 170/256\n",
            "22039/22039 [==============================] - 51s 2ms/step - loss: 0.6131 - accuracy: 0.7403 - val_loss: 0.6291 - val_accuracy: 0.7349\n",
            "Epoch 171/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6130 - accuracy: 0.7408 - val_loss: 0.6175 - val_accuracy: 0.7377\n",
            "Epoch 172/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6131 - accuracy: 0.7401 - val_loss: 0.6414 - val_accuracy: 0.7285\n",
            "Epoch 173/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6133 - accuracy: 0.7404 - val_loss: 0.6205 - val_accuracy: 0.7384\n",
            "Epoch 174/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6126 - accuracy: 0.7409 - val_loss: 0.6274 - val_accuracy: 0.7354\n",
            "Epoch 175/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6122 - accuracy: 0.7408 - val_loss: 0.6300 - val_accuracy: 0.7357\n",
            "Epoch 176/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6127 - accuracy: 0.7406 - val_loss: 0.6401 - val_accuracy: 0.7236\n",
            "Epoch 177/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6120 - accuracy: 0.7411 - val_loss: 0.6327 - val_accuracy: 0.7325\n",
            "Epoch 178/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6121 - accuracy: 0.7406 - val_loss: 0.6227 - val_accuracy: 0.7371\n",
            "Epoch 179/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6118 - accuracy: 0.7415 - val_loss: 0.6217 - val_accuracy: 0.7386\n",
            "Epoch 180/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6117 - accuracy: 0.7413 - val_loss: 0.6444 - val_accuracy: 0.7279\n",
            "Epoch 181/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6121 - accuracy: 0.7413 - val_loss: 0.6269 - val_accuracy: 0.7401\n",
            "Epoch 182/256\n",
            "22039/22039 [==============================] - 53s 2ms/step - loss: 0.6117 - accuracy: 0.7414 - val_loss: 0.6197 - val_accuracy: 0.7350\n",
            "Epoch 183/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6118 - accuracy: 0.7412 - val_loss: 0.6116 - val_accuracy: 0.7420\n",
            "Epoch 184/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6114 - accuracy: 0.7415 - val_loss: 0.6309 - val_accuracy: 0.7351\n",
            "Epoch 185/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6114 - accuracy: 0.7412 - val_loss: 0.6198 - val_accuracy: 0.7439\n",
            "Epoch 186/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6115 - accuracy: 0.7413 - val_loss: 0.6248 - val_accuracy: 0.7361\n",
            "Epoch 187/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6117 - accuracy: 0.7413 - val_loss: 0.6215 - val_accuracy: 0.7364\n",
            "Epoch 188/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6114 - accuracy: 0.7415 - val_loss: 0.6198 - val_accuracy: 0.7383\n",
            "Epoch 189/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6103 - accuracy: 0.7421 - val_loss: 0.6194 - val_accuracy: 0.7398\n",
            "Epoch 190/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6108 - accuracy: 0.7417 - val_loss: 0.6285 - val_accuracy: 0.7341\n",
            "Epoch 191/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6104 - accuracy: 0.7421 - val_loss: 0.6194 - val_accuracy: 0.7402\n",
            "Epoch 192/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6105 - accuracy: 0.7419 - val_loss: 0.6216 - val_accuracy: 0.7420\n",
            "Epoch 193/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6107 - accuracy: 0.7419 - val_loss: 0.6123 - val_accuracy: 0.7436\n",
            "Epoch 194/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6102 - accuracy: 0.7419 - val_loss: 0.6272 - val_accuracy: 0.7367\n",
            "Epoch 195/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6104 - accuracy: 0.7419 - val_loss: 0.6245 - val_accuracy: 0.7349\n",
            "Epoch 196/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6105 - accuracy: 0.7419 - val_loss: 0.6125 - val_accuracy: 0.7421\n",
            "Epoch 197/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6101 - accuracy: 0.7423 - val_loss: 0.6291 - val_accuracy: 0.7369\n",
            "Epoch 198/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6097 - accuracy: 0.7421 - val_loss: 0.6245 - val_accuracy: 0.7391\n",
            "Epoch 199/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6097 - accuracy: 0.7422 - val_loss: 0.6216 - val_accuracy: 0.7389\n",
            "Epoch 200/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6101 - accuracy: 0.7418 - val_loss: 0.6114 - val_accuracy: 0.7421\n",
            "Epoch 201/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6096 - accuracy: 0.7422 - val_loss: 0.6211 - val_accuracy: 0.7377\n",
            "Epoch 202/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6089 - accuracy: 0.7428 - val_loss: 0.6153 - val_accuracy: 0.7413\n",
            "Epoch 203/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6088 - accuracy: 0.7427 - val_loss: 0.6273 - val_accuracy: 0.7348\n",
            "Epoch 204/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6095 - accuracy: 0.7426 - val_loss: 0.6176 - val_accuracy: 0.7417\n",
            "Epoch 205/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6097 - accuracy: 0.7424 - val_loss: 0.6244 - val_accuracy: 0.7374\n",
            "Epoch 206/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6090 - accuracy: 0.7427 - val_loss: 0.6298 - val_accuracy: 0.7346\n",
            "Epoch 207/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6102 - accuracy: 0.7426 - val_loss: 0.6445 - val_accuracy: 0.7323\n",
            "Epoch 208/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6090 - accuracy: 0.7427 - val_loss: 0.6231 - val_accuracy: 0.7430\n",
            "Epoch 209/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6091 - accuracy: 0.7424 - val_loss: 0.6156 - val_accuracy: 0.7402\n",
            "Epoch 210/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6083 - accuracy: 0.7431 - val_loss: 0.6359 - val_accuracy: 0.7298\n",
            "Epoch 211/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6082 - accuracy: 0.7435 - val_loss: 0.6180 - val_accuracy: 0.7416\n",
            "Epoch 212/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6087 - accuracy: 0.7429 - val_loss: 0.6141 - val_accuracy: 0.7445\n",
            "Epoch 213/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6083 - accuracy: 0.7431 - val_loss: 0.6282 - val_accuracy: 0.7357\n",
            "Epoch 214/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6095 - accuracy: 0.7428 - val_loss: 0.6167 - val_accuracy: 0.7418\n",
            "Epoch 215/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6085 - accuracy: 0.7429 - val_loss: 0.6336 - val_accuracy: 0.7329\n",
            "Epoch 216/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6085 - accuracy: 0.7426 - val_loss: 0.6322 - val_accuracy: 0.7352\n",
            "Epoch 217/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6075 - accuracy: 0.7434 - val_loss: 0.6096 - val_accuracy: 0.7466\n",
            "Epoch 218/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6080 - accuracy: 0.7430 - val_loss: 0.6259 - val_accuracy: 0.7361\n",
            "Epoch 219/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6077 - accuracy: 0.7434 - val_loss: 0.6204 - val_accuracy: 0.7415\n",
            "Epoch 220/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6079 - accuracy: 0.7434 - val_loss: 0.6268 - val_accuracy: 0.7354\n",
            "Epoch 221/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6076 - accuracy: 0.7437 - val_loss: 0.6329 - val_accuracy: 0.7344\n",
            "Epoch 222/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6074 - accuracy: 0.7435 - val_loss: 0.6306 - val_accuracy: 0.7321\n",
            "Epoch 223/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6076 - accuracy: 0.7437 - val_loss: 0.6335 - val_accuracy: 0.7334\n",
            "Epoch 224/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6074 - accuracy: 0.7434 - val_loss: 0.6196 - val_accuracy: 0.7393\n",
            "Epoch 225/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6080 - accuracy: 0.7432 - val_loss: 0.6121 - val_accuracy: 0.7428\n",
            "Epoch 226/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6069 - accuracy: 0.7434 - val_loss: 0.6150 - val_accuracy: 0.7398\n",
            "Epoch 227/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6072 - accuracy: 0.7436 - val_loss: 0.6204 - val_accuracy: 0.7415\n",
            "Epoch 228/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6065 - accuracy: 0.7441 - val_loss: 0.6108 - val_accuracy: 0.7421\n",
            "Epoch 229/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6070 - accuracy: 0.7442 - val_loss: 0.6156 - val_accuracy: 0.7401\n",
            "Epoch 230/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6069 - accuracy: 0.7437 - val_loss: 0.6193 - val_accuracy: 0.7373\n",
            "Epoch 231/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6071 - accuracy: 0.7437 - val_loss: 0.6324 - val_accuracy: 0.7348\n",
            "Epoch 232/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6071 - accuracy: 0.7436 - val_loss: 0.6152 - val_accuracy: 0.7422\n",
            "Epoch 233/256\n",
            "22039/22039 [==============================] - 48s 2ms/step - loss: 0.6068 - accuracy: 0.7439 - val_loss: 0.6097 - val_accuracy: 0.7448\n",
            "Epoch 234/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6058 - accuracy: 0.7441 - val_loss: 0.6229 - val_accuracy: 0.7381\n",
            "Epoch 235/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6068 - accuracy: 0.7437 - val_loss: 0.6404 - val_accuracy: 0.7323\n",
            "Epoch 236/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6074 - accuracy: 0.7441 - val_loss: 0.6269 - val_accuracy: 0.7350\n",
            "Epoch 237/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6065 - accuracy: 0.7441 - val_loss: 0.6154 - val_accuracy: 0.7452\n",
            "Epoch 238/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6065 - accuracy: 0.7443 - val_loss: 0.6437 - val_accuracy: 0.7256\n",
            "Epoch 239/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6064 - accuracy: 0.7438 - val_loss: 0.6302 - val_accuracy: 0.7369\n",
            "Epoch 240/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6058 - accuracy: 0.7445 - val_loss: 0.6219 - val_accuracy: 0.7397\n",
            "Epoch 241/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6060 - accuracy: 0.7444 - val_loss: 0.6368 - val_accuracy: 0.7299\n",
            "Epoch 242/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6062 - accuracy: 0.7445 - val_loss: 0.6206 - val_accuracy: 0.7426\n",
            "Epoch 243/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6054 - accuracy: 0.7447 - val_loss: 0.6184 - val_accuracy: 0.7387\n",
            "Epoch 244/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6060 - accuracy: 0.7446 - val_loss: 0.6234 - val_accuracy: 0.7381\n",
            "Epoch 245/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6051 - accuracy: 0.7447 - val_loss: 0.6149 - val_accuracy: 0.7416\n",
            "Epoch 246/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6055 - accuracy: 0.7444 - val_loss: 0.6158 - val_accuracy: 0.7403\n",
            "Epoch 247/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6057 - accuracy: 0.7446 - val_loss: 0.6108 - val_accuracy: 0.7377\n",
            "Epoch 248/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6060 - accuracy: 0.7445 - val_loss: 0.6135 - val_accuracy: 0.7383\n",
            "Epoch 249/256\n",
            "22039/22039 [==============================] - 50s 2ms/step - loss: 0.6052 - accuracy: 0.7447 - val_loss: 0.6169 - val_accuracy: 0.7433\n",
            "Epoch 250/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6057 - accuracy: 0.7447 - val_loss: 0.6158 - val_accuracy: 0.7424\n",
            "Epoch 251/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6047 - accuracy: 0.7450 - val_loss: 0.6315 - val_accuracy: 0.7406\n",
            "Epoch 252/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6047 - accuracy: 0.7451 - val_loss: 0.6045 - val_accuracy: 0.7483\n",
            "Epoch 253/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6054 - accuracy: 0.7450 - val_loss: 0.6178 - val_accuracy: 0.7400\n",
            "Epoch 254/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6051 - accuracy: 0.7455 - val_loss: 0.6205 - val_accuracy: 0.7389\n",
            "Epoch 255/256\n",
            "22039/22039 [==============================] - 49s 2ms/step - loss: 0.6046 - accuracy: 0.7454 - val_loss: 0.6132 - val_accuracy: 0.7397\n",
            "Epoch 256/256\n",
            "22039/22039 [==============================] - 52s 2ms/step - loss: 0.6049 - accuracy: 0.7451 - val_loss: 0.6176 - val_accuracy: 0.7433\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             multiple                  1536      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             multiple                  387       \n",
            "=================================================================\n",
            "Total params: 51,459\n",
            "Trainable params: 51,459\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Evaluate on test data\n",
            "4756/4756 [==============================] - 7s 1ms/step - loss: 0.6235 - accuracy: 0.7381\n",
            "test loss, test acc: [0.6234815120697021, 0.738135039806366]\n",
            "Generate predictions for 3 samples\n",
            "predictions shape: (3, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}